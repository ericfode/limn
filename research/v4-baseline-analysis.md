# v4 Baseline on v5 Clean Evaluation

> hyp tes | dat cle | tru eme
> *(hypothesis tested | data cleaned | truth emerges)*

**Date:** 2026-02-05
**Model:** Qwen2.5-0.5B full fine-tune (494M params, v4 training data)
**Eval:** v5 framework — decontaminated splits, 3-tier compositional generalization

---

## Results Summary

### v4 Category Scores (retained tests)

| Category | Score | Notes |
|----------|-------|-------|
| Vocabulary Recall | 65% (13/20) | Reverse lookup (English → Limn) |
| Operator Semantics | 88% (7/8) | Explains what operators do |
| False Friends | 67% (4/6) | Detects bat≠battle, dan≠danger etc. |
| Algebraic Invariance | **100%** (25/25) | Perfect commutativity understanding |

### Compositional Generalization Tiers

| Tier | Avg Score | Pass Rate | Description |
|------|-----------|-----------|-------------|
| Tier 1 (Interpolation) | 0.6904 | 77% (23/30) | Seen patterns + words |
| Tier 2 (Compositional) | 0.6753 | 97% (29/30) | Seen ops, UNSEEN combos |
| Tier 3 (Productive) | 0.6732 | 100% (30/30) | Seen ops, UNSEEN words |

**Overfitting penalty (T2-T1):** -0.0151
**Memorization penalty (T3-T1):** -0.0172

### Per-Operator Accuracy (from DB expressions)

| Op | Name | Score | Notes |
|----|------|-------|-------|
| @ | projection | 85% (17/20) | |
| * | interference | 85% (17/20) | |
| ^ | gradient | 80% (16/20) | Weakest — intensity semantics |
| \ | subtraction | 100% (18/18) | |
| ± | superposition | 100% (10/10) | Small sample — less reliable |
| : | conditional | 94% (15/16) | |

---

## Analysis

### Finding 1: Near-flat generalization curve

The composite scores across tiers are nearly flat: 0.69 → 0.68 → 0.67. The drop from Tier 1 to Tier 3 is only -0.017. Two possible explanations:

**(a) The model genuinely generalizes compositional operators.** It has learned what `@`, `*`, etc. *do* rather than memorizing specific word pairs. This would be good news for H1.

**(b) The scoring function doesn't discriminate hard enough.** The Tier 2/3 eval items have formulaic answers generated by our script (e.g., "applying projection (@): meaning1 @ meaning2"). The keyword overlap dimension (25% of composite) likely matches well against the model's tendency to repeat operator names. Meanwhile Tier 1 uses heterogeneous val examples where keyword matching is harder.

**Evidence for (b):** Pass rates are *inverted* — Tier 3 (100%) > Tier 2 (97%) > Tier 1 (77%). If the model truly generalizes better on harder tasks, that contradicts basic ML principles. More likely, the scoring function is easier to satisfy on formulaic items.

**Verdict:** The composite scores are the reliable metric (nearly flat = decent generalization), but the pass rates are methodological artifacts. The scoring function needs refinement for future iterations.

### Finding 2: Algebraic invariance is solved

25/25 on commutativity tests. The model perfectly distinguishes commutative operators (`*`, `±`) from non-commutative ones (`@`, `\`, `:`). This was a v4 training focus (dedicated commutativity probes) and it paid off.

### Finding 3: Per-operator performance is surprisingly uniform

Range: 80% (gradient) to 100% (subtraction, superposition). No catastrophic failures on any operator, even ± with its severely underrepresented training data. However:

- **± sample is tiny (10)** — 100% on 10 items ≠ proven competence
- **^ at 80%** is the weakest — gradient semantics (continuous intensity) may require different training emphasis
- **\ at 100%** suggests "A without B" is an easy pattern to learn (clear English mapping)

### Finding 4: Vocabulary recall at 65% is a real baseline

On clean eval with no leakage, the model correctly recalls 13/20 words from English descriptions. Previously, with 62% data leakage, this number was likely inflated. 65% is the honest number.

### Finding 5: False friends at 67% needs improvement

2 of 6 false friends were missed. These are the most dangerous errors (bat="bathroom" not "battle") and 67% means the model will mislead users 1/3 of the time.

---

## Implications for v5

1. **H1 (operators vs memorization):** Tentatively positive — tier scores are nearly flat. But need harder scoring to be confident. The eval framework exists; the scoring function needs to be more discriminating.

2. **H2 (data leakage):** CONFIRMED RESOLVED. Zero leakage in v5 splits. The 65% vocabulary recall is the honest baseline.

3. **Scoring function is the next bottleneck.** The composite score works for comparing models/tiers but lacks granularity. Phase 4's embedding-based semantic reward would be a much better scoring mechanism.

4. **More ± test data needed.** 10 examples is insufficient. The v5 generated data (632 ± training examples) should help, but eval set needs expansion too.

5. **Gradient operator (^) needs targeted work.** At 80% it's the weakest. The continuous intensity semantics (0.0-1.0) may require more diverse training examples.

---

## Next Steps

1. Refine scoring function — reduce keyword overlap weight, add structural parsing score
2. Expand Tier 2/3 eval sets (200 → 500+, with more diverse answer formats)
3. Train v5 model on decontaminated data, compare against this baseline
4. Begin Phase 3 (information theory) — entropy analysis doesn't need model changes

---

```limn
bas set | tru kno | nex ste cle
> baseline set | truth known | next steps clear
```

*— Lex*
